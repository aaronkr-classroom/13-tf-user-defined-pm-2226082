# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oaQ4iJroCxD-XIPHnDKt0il_r19G8NB1
"""

def build(self, input_shape):
    self.alpha = self.add_weight(
        shape=input_shape[-1:],  # 마지막 축 기준
        initializer="ones",  # α 초기값: 1
        trainable=True       # 학습 가능한 매개변수로 설정
    )
    self.beta = self.add_weight(
        shape=input_shape[-1:],  # 마지막 축 기준
        initializer="zeros",  # β 초기값: 0
        trainable=True        # 학습 가능한 매개변수로 설정
    )

def call(self, inputs):
    # 평균 μ와 분산 σ^2 계산
    mean, variance = tf.nn.moments(inputs, axes=-1, keepdims=True)

    # 정규화 수행
    normalized = (inputs - mean) / tf.sqrt(variance + 1e-5)  # ε = 1e-5

    # α와 β를 적용
    return self.alpha * normalized + self.beta

# 기본 LayerNormalization
keras_layer_norm = tf.keras.layers.LayerNormalization()

# 사용자 정의 LayerNormalization
class CustomLayerNormalization(tf.keras.layers.Layer):
    def __init__(self):
        super(CustomLayerNormalization, self).__init__()

    def build(self, input_shape):
        self.alpha = self.add_weight(
            shape=input_shape[-1:], initializer="ones", trainable=True
        )
        self.beta = self.add_weight(
            shape=input_shape[-1:], initializer="zeros", trainable=True
        )

    def call(self, inputs):
        mean, variance = tf.nn.moments(inputs, axes=-1, keepdims=True)
        normalized = (inputs - mean) / tf.sqrt(variance + 1e-5)
        return self.alpha * normalized + self.beta

custom_layer_norm = CustomLayerNormalization()

# 테스트 데이터
import numpy as np
x = tf.constant(np.random.random((2, 5)), dtype=tf.float32)

# 출력 비교
keras_output = keras_layer_norm(x)
custom_output = custom_layer_norm(x)

# 동일한지 확인
print("두 출력이 동일한가?", tf.reduce_all(tf.math.abs(keras_output - custom_output) < 1e-5))

import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

# 데이터 로드 및 전처리
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # 정규화

# 모델 정의
model = Sequential([
    Flatten(input_shape=(28, 28)),  # 입력 데이터는 2D로 들어오므로 Flatten
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# 사용자 정의 학습률 적용을 위한 옵티마이저 구성
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# 모델 컴파일
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 사용자 정의 훈련 루프
epochs = 10
batch_size = 64

for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")
    epoch_loss = 0
    epoch_accuracy = 0
    batches = 0

    for i in range(0, len(x_train), batch_size):
        x_batch = x_train[i:i+batch_size]
        y_batch = y_train[i:i+batch_size]

        # 모델 입력 데이터 reshape (Optional)
        x_batch = x_batch.reshape(-1, 28, 28)  # 만약 Flatten 층이 있다면 필요 없음

        # 훈련 단계
        history = model.train_on_batch(x_batch, y_batch)
        batch_loss, batch_accuracy = history
        epoch_loss += batch_loss
        epoch_accuracy += batch_accuracy
        batches += 1

    # 에포크 평균 손실 및 정확도
    epoch_loss /= batches
    epoch_accuracy /= batches
    print(f"Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}")

# 테스트 데이터로 평가
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

# 상위 층과 하위 층에 다른 학습률 적용
lower_layers_optimizer = Adam(learning_rate=0.001)
upper_layers_optimizer = Adam(learning_rate=0.0001)

# 층별 학습률 적용을 위한 옵티마이저 설정
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 다시 훈련
model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))